{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from auc_tools import standard_auc, monte_carlo_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Visualize the zoo of response curves\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create time points\n",
    "x = np.linspace(0, 120, 500)\n",
    "\n",
    "# Create a figure for plotting\n",
    "plt.figure(figsize=(12, 16))\n",
    "\n",
    "# Generate and plot different response curves\n",
    "curve_types = ['skewed_gaussian', 'biexponential', 'gamma', 'lognormal', \n",
    "               'weibull', 'bateman', 'inverse_gaussian']\n",
    "\n",
    "sampled_curves = sample_curves(parameter_bounds)\n",
    "\n",
    "\n",
    "for i, curve_type in enumerate(curve_types):\n",
    "\n",
    "    plt.subplot(len(curve_types), 1, i+1)\n",
    "    plt.title(f\"{curve_type.replace('_', ' ').capitalize()}\")\n",
    "\n",
    "    # Sample random parameters within bounds\n",
    "    bounds = parameter_bounds[curve_type]\n",
    "    params = {}\n",
    "    for param, (min_val, max_val) in bounds.items():\n",
    "        params[param] = np.random.uniform(min_val, max_val)\n",
    "    \n",
    "    for params in sampled_curves[curve_type]:\n",
    "        response, start, end = generate_response_curve(x, curve_type=curve_type, params=params)\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(x, response)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        ax = plt.gca()\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.xlabel('Time (min)', fontsize=12)\n",
    "plt.gcf().text(-.01, 0.5, 'Density', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/response_curves_zoo.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actual response curves using the sampled parameters\n",
    "response_curves_set = []\n",
    "\n",
    "ctr = 0\n",
    "for num_participants in range(5, 42, 5):  # Participants from 5 to 41 with step size of 5\n",
    "    for num_datapoints in range(4, 11):  # Datapoints from 4 to 10\n",
    "        for curve_type, param_list in sampled_curves.items():\n",
    "            for index, params in enumerate(param_list):\n",
    "                # Generate the true response curve\n",
    "                response, support_start, support_end = generate_response_curve(\n",
    "                    x, curve_type=curve_type, params=params, threshold=0.01\n",
    "                )\n",
    "                \n",
    "                # Define gridpoints within the active support\n",
    "                gridpoints = np.linspace(support_start, support_end, num_datapoints)\n",
    "                #gridpoints = find_representative_points(x, response, num_datapoints)\n",
    "                \n",
    "                # Evaluate the response at the gridpoints\n",
    "                true_values = np.interp(gridpoints, x, response)\n",
    "                \n",
    "                # Generate random values around the true values with 20% standard deviation\n",
    "                random_values = np.random.normal(\n",
    "                    loc=true_values, scale=true_values * 0.2, size=(num_participants, len(gridpoints))\n",
    "                )\n",
    "                \n",
    "                # Calculate mean and standard deviation of the responses\n",
    "                mean_responses = random_values.mean(axis=0)\n",
    "                std_responses = random_values.std(axis=0)\n",
    "                \n",
    "                # Calculate the true AUC over the support\n",
    "                true_auc = np.trapz(response[(x >= support_start) & (x <= support_end)], x[(x >= support_start) & (x <= support_end)])\n",
    "\n",
    "\n",
    "                # Append the result to the response_curves_set\n",
    "                response_curves_set.append({\n",
    "                    'index': index,\n",
    "                    'curve_type': curve_type,\n",
    "                    'parameters': params,\n",
    "                    'x_vals': gridpoints,\n",
    "                    'num_participants': num_participants,\n",
    "                    'mean_responses': mean_responses,\n",
    "                    'std_responses': std_responses,\n",
    "                    'support_start': support_start,\n",
    "                    'support_end': support_end,\n",
    "                    'true_auc': true_auc,\n",
    "                })\n",
    "\n",
    "                if ctr % 100 == 0:\n",
    "                    print(f\"Generated {ctr} of 3920 curves \")\n",
    "                ctr += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part we should be able to run in a loop, sweeping over number of participants and datapoints\n",
    "\n",
    "results = {\n",
    "    \"grid_points_in_graph\": [],\n",
    "    \"participants\": [],\n",
    "    \"curve_type\": [],\n",
    "    \"parameters\": [],\n",
    "    \"standard_auc_mean\": [],\n",
    "    \"standard_auc_lower_bound\": [],\n",
    "    \"standard_auc_upper_bound\": [],\n",
    "    \"mc_auc_mean\": [],\n",
    "    \"mc_auc_std\": [],\n",
    "    \"bayesian_auc_mean\": [],     # New\n",
    "    \"bayesian_hdi_low\": [],      # New\n",
    "    \"bayesian_hdi_high\": [],     # New\n",
    "    \"bayesian_iterations\": [],    # New\n",
    "    \"current_grid\": [],\n",
    "    \"means\": [],\n",
    "    \"stds\": [],\n",
    "    \"true_auc\": []\n",
    "}\n",
    "\n",
    "for ctr, curve in enumerate(response_curves_set):\n",
    "    current_grid, means, stds = curve['x_vals'], curve['mean_responses'], curve['std_responses']\n",
    "\n",
    "    # Calculate standard AUC\n",
    "    standard_auc_mean, auc_lower_bound, auc_upper_bound = standard_auc(\n",
    "        current_grid, means, stds, curve['num_participants']\n",
    "    )\n",
    "\n",
    "    # Calculate Monte Carlo AUC\n",
    "    mc_auc_mean, mc_auc_std = monte_carlo_auc(current_grid, means, stds, n_sim=1000)\n",
    "    \n",
    "    # Calculate Bayesian AUC\n",
    "    posterior_samples, bayes_mean, bayes_hdi_low, bayes_hdi_high, bayes_iterations = bayesian_auc(\n",
    "        current_grid, means, stds, n_iterations=1000, convergence_threshold=0.000001\n",
    "    )\n",
    "\n",
    "    # Store all results\n",
    "    results[\"standard_auc_mean\"].append(standard_auc_mean)\n",
    "    results[\"grid_points_in_graph\"].append(len(current_grid))\n",
    "    results[\"participants\"].append(curve['num_participants'])\n",
    "    results[\"curve_type\"].append(curve['curve_type'])\n",
    "    results[\"parameters\"].append(curve['parameters'])       \n",
    "    results[\"standard_auc_lower_bound\"].append(auc_lower_bound)\n",
    "    results[\"standard_auc_upper_bound\"].append(auc_upper_bound)\n",
    "    results[\"mc_auc_mean\"].append(mc_auc_mean)\n",
    "    results[\"mc_auc_std\"].append(mc_auc_std)\n",
    "    results[\"bayesian_auc_mean\"].append(bayes_mean)        # New\n",
    "    results[\"bayesian_hdi_low\"].append(bayes_hdi_low)      # New\n",
    "    results[\"bayesian_hdi_high\"].append(bayes_hdi_high)    # New\n",
    "    results[\"bayesian_iterations\"].append(bayes_iterations) # New\n",
    "    results[\"current_grid\"].append(current_grid)\n",
    "    results[\"means\"].append(means)\n",
    "    results[\"stds\"].append(stds)\n",
    "    results[\"true_auc\"].append(curve['true_auc'])\n",
    "    \n",
    "    if ctr % 100 == 0:\n",
    "        print(f\"Processed {ctr} curves out of {len(response_curves_set)}\")\n",
    "\n",
    "# Create DataFrame and add error metrics\n",
    "df = pd.DataFrame(results) \n",
    "\n",
    "# Add relative error and coverage for all methods\n",
    "df[\"mc_relative_error\"] = abs(df[\"mc_auc_mean\"] - df[\"true_auc\"]) / df[\"true_auc\"]\n",
    "df[\"standard_relative_error\"] = abs(df[\"standard_auc_mean\"] - df[\"true_auc\"]) / df[\"true_auc\"]\n",
    "df[\"bayesian_relative_error\"] = abs(df[\"bayesian_auc_mean\"] - df[\"true_auc\"]) / df[\"true_auc\"]\n",
    "\n",
    "# Calculate standard deviations for coverage calculation\n",
    "df['standard_auc_simple_std'] = (df['standard_auc_mean'] - df['standard_auc_lower_bound'] + \n",
    "                                df['standard_auc_upper_bound'] - df['standard_auc_mean']) / 2\n",
    "df['bayesian_auc_std'] = (df['bayesian_hdi_high'] - df['bayesian_hdi_low']) / 3.29  # 89% HDI\n",
    "\n",
    "# Add coverage indicators for all methods\n",
    "df[\"mc_coverage\"] = (\n",
    "    (df[\"mc_auc_mean\"] - 1.96 * df[\"mc_auc_std\"] <= df[\"true_auc\"]) &\n",
    "    (df[\"mc_auc_mean\"] + 1.96 * df[\"mc_auc_std\"] >= df[\"true_auc\"])\n",
    ").astype(int)\n",
    "\n",
    "df[\"standard_coverage\"] = (\n",
    "    (df[\"standard_auc_mean\"] - 1.96 * df[\"standard_auc_simple_std\"] <= df[\"true_auc\"]) &\n",
    "    (df[\"standard_auc_mean\"] + 1.96 * df[\"standard_auc_simple_std\"] >= df[\"true_auc\"])\n",
    ").astype(int)\n",
    "\n",
    "df[\"bayesian_coverage\"] = (\n",
    "    (df[\"bayesian_hdi_low\"] <= df[\"true_auc\"]) &\n",
    "    (df[\"bayesian_hdi_high\"] >= df[\"true_auc\"])\n",
    ").astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../results/auc_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc560a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auc_tools import compare_methods_performance\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store all results\n",
    "all_results = []\n",
    "\n",
    "# Analyze each curve type separately\n",
    "for curve_type in df['curve_type'].unique():\n",
    "    curve_data = df[df['curve_type'] == curve_type]\n",
    "    \n",
    "    # Create dictionary of methods with their estimates and standard errors\n",
    "    methods = {\n",
    "        'Standard': (\n",
    "            curve_data['standard_auc_mean'].values,\n",
    "            curve_data['standard_auc_simple_std'].values\n",
    "        ),\n",
    "        'Monte Carlo': (\n",
    "            curve_data['mc_auc_mean'].values,\n",
    "            curve_data['mc_auc_std'].values\n",
    "        ),\n",
    "        'Bayesian': (\n",
    "            curve_data['bayesian_auc_mean'].values,\n",
    "            curve_data['bayesian_auc_std'].values\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    results = compare_methods_performance(curve_data['true_auc'].values, methods)\n",
    "    \n",
    "    # Add curve type as index name\n",
    "    results.index.name = 'Method'\n",
    "    results = results.reset_index()\n",
    "    results.insert(0, 'Curve Type', curve_type)\n",
    "    \n",
    "    all_results.append(results)\n",
    "\n",
    "# Combine all results\n",
    "final_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Display results in a nicely formatted table\n",
    "print(\"Performance Metrics by Curve Type and Method\")\n",
    "print(\"(Values shown as: Estimate (Monte Carlo Standard Error))\")\n",
    "print(\"\\nBias, Empirical SE, RMSE, and Coverage (95% CI)\")\n",
    "print(\"=\"*80)\n",
    "print(final_results.to_string(index=False))\n",
    "\n",
    "# Save results to CSV\n",
    "final_results.to_csv('performance_metrics.csv', index=False)\n",
    "\n",
    "# Create separate tables for each metric for easier reporting\n",
    "metrics = ['bias', 'empirical_se', 'rmse', 'coverage']\n",
    "for metric in metrics:\n",
    "    print(f\"\\n{metric.upper()} by Curve Type and Method:\")\n",
    "    print(\"=\"*50)\n",
    "    pivot_table = final_results.pivot(\n",
    "        index='Curve Type',\n",
    "        columns='Method',\n",
    "        values=metric\n",
    "    )\n",
    "    print(pivot_table.to_string())\n",
    "    # Save individual metric tables\n",
    "    pivot_table.to_csv(f'../results/{metric}_by_method.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
